---
title: "Time Series Tests JOINED DATA"
author: "Michael Weisner"
date: "3/22/2020"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE,warning=FALSE)
library(tinytex)
library(kableExtra)
library(qwraps2)
library(stringr)
library(httr)
library(data.table)
library(lubridate)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(ggthemes)

library(tseries)
library(gridExtra)
library(forecast)
library(olsrr)

library(caret)
library(doMC)

library(lmtest)
library(sandwich)

library(kableExtra)


options(qwraps2_markup = "markdown")
```

```{r}
theme_fivethirtyeight_mw <- function(base_size = 12, base_family = "sans") {
  colors <- deframe(ggthemes::ggthemes_data[["fivethirtyeight"]])
  (theme_foundation(base_size = base_size, base_family = base_family)
   + theme(
     line = element_line(colour = "black"),
     rect = element_rect(fill = colors["Light Gray"],
                         linetype = 0, colour = NA),
     text = element_text(colour = colors["Dark Gray"]),
     axis.ticks = element_blank(),
     axis.line = element_blank(),
     legend.background = element_rect(),
     legend.position = "bottom",
     legend.direction = "horizontal",
     legend.box = "vertical",
     panel.grid = element_line(colour = NULL),
     panel.grid.major =
       element_line(colour = colors["Medium Gray"]),
     panel.grid.minor = element_blank(),
     # unfortunately, can't mimic subtitles TODO!
     plot.title = element_text(hjust = 0, size = rel(1.5), face = "bold"),
     plot.margin = unit(c(1, 1, 1, 1), "lines"),
     strip.background = element_rect()))
}
```

## Data

```{r}
snp_complete_joined_noNA_noRec <- read_csv("data/JOINED_snp_complete_noNA_noRec_noclose.csv")

snp_complete_noNA_close <- read_csv("data/JOINED_snp_complete_noNA_noRec_close.csv")
```

## Summary Statistics

```{r}
snp_no_dummies <- snp_complete_joined_noNA_noRec %>%
  select(c(Return_perc, VIX_Close, pm2.5_Daily_Sum_UGPM3, ozone_Daily_8hr_Average_PPM, CO_Daily_8hr_Average_PPM, mean_hr_dewpoint_tempF, mean_hr_tempF, mean_hr_pressure_psi, mean_hr_windspeed_knots, mean_hr_precip_in, mean_hr_clouds_percent))

variable_descriptive_stats <- psych::describe(snp_no_dummies)

variable_descriptive_stats <- variable_descriptive_stats %>%
  select(-c(vars, trimmed, mad, skew, kurtosis))

rownames(variable_descriptive_stats) <- c("Return Percentage", "VIX Closing", "PM2.5 Daily Sum (ug per cubic meter)", "Ozone Daily 8 Hour Average (PPM)", "CO Daily 8 Hour Average (PPM)", "Mean Hourly Dewpoint (F)", "Mean Hourly Temperature (F)", "Mean Hourly Pressure (PSI)", "Mean Hourly Windspeed (Knots)", "Mean Hourly Precipitation (inches)", "Mean Hourly Cloud Coverage (Percentage)")

variable_descriptive_stats %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped","condensed"))
```



## Test with ts()

```{r}
#full_dates <- seq(as.Date("2008-07-10"), as.Date("2020-01-01"), by="days")

full_dates <- seq(as.Date("2006-12-27"), as.Date("2020-01-01"), by="days")
full_dates <- as.data.frame(full_dates)
colnames(full_dates)  <- "Date"

# 2019-11-29 is the current full amount of days

# snp_ols <- snp_complete_joined_noNA_noRec %>%
#   mutate(Close_lag1 = lag(x = snp_complete_joined_noNA_noRec$Close, n = 1L)) 
# 
# snp_ols <- snp_ols %>%
#   mutate(Close_lag2 = lag(x = snp_ols$Close_lag1, n = 1L))

snp_ols <- snp_complete_joined_noNA_noRec

full_dates <- left_join(full_dates, snp_ols)
```

### Remove Weekends from full_dates
```{r}
full_dates_weeks <- full_dates %>%
  mutate(weekdays = weekdays(full_dates$Date)) %>%
  filter(weekdays != "Saturday" & weekdays != "Sunday")

incompletes <- full_dates_weeks[!complete.cases(full_dates_weeks), ]
completes <- full_dates_weeks[complete.cases(full_dates_weeks), ]

for_keras <- completes %>%
  select(-c(weekdays, Return_perc_lag1, Return_perc_lag2, pm2.5_Daily_Lag1, pm2.5_Daily_Lag2)) %>%
  rename(index = Date) %>%
  rename(value = Return_perc)

write_csv(x = for_keras, path = "LSTM/keras_nolag_completes.csv")
nrow(completes)
nrow(incompletes)
```
Out of 3396 total weekdays (excluding the financial crisis of 2008) we have 2922 complete cases and 474 missing cases, or about 14% missing days. These may be somewhat accounted for by federal holidays.


```{r}
simple <- select(snp_complete_joined_noNA_noRec, c("Date", "Return_perc"))

simple_ts <- ts(simple)
```


## TS with S&P

### Decomposition
```{r}
snp_ts <- ts(full_dates[, 2], start = c(2006, 361), end = c(2020, 1), frequency = 365)

# 2019-11-29 is the last day in the sample annoyingly

snp_ts <- na.remove(snp_ts)

# Decompose
decomposedRes <- decompose(snp_ts, type="additive")

plot(decomposedRes)
```


### ACF & PACF
```{r}
acfRes <- acf(snp_ts, type = "correlation")
```

```{r}
pacfRes <- pacf(snp_ts)
```

#### Full ACF Test
```{r}
tidy_acf <- function(data, value, lags = 0:20) {
    
    value_expr <- enquo(value)
    
    acf_values <- data %>%
        pull(value) %>%
        acf(lag.max = tail(lags, 1), plot = FALSE) %>%
        .$acf %>%
        .[,,1]
    
    ret <- tibble(acf = acf_values) %>%
        rowid_to_column(var = "lag") %>%
        mutate(lag = lag - 1) %>%
        filter(lag %in% lags)
    
    return(ret)
}

max_lag <- nrow(snp_complete_joined_noNA_noRec) - 1

snp_complete_joined_noNA_noRec %>%
    tidy_acf("Return_perc", lags = 0:max_lag) %>%
    ggplot(aes(lag, acf)) +
    geom_segment(aes(xend = lag, yend = 0)) +
    theme_fivethirtyeight_mw() +
    labs(title = "ACF: S&P 500 Return")
```

#### Seasonality
It is already well documented that the stock market, including the S&P 500, follows seasonal trends (Saunders, 1993; Kamstra, Kramer, Levi 2012). To handle seasonality effects we included dummy variables for each week of the year (Kamstra et al, 2012).

#### Heteroskedasticity
```{r}
model <- lm(Return_perc ~ Date, data = snp_complete_joined_noNA_noRec)

ols_test_breusch_pagan(model)
```


#### Testing for Autocorrelation
First we must do a box test if our model has a "unit root" - which would indicate autocorrelation. Autocorrelation is a problem in time-series analysis, as it means that the data has correlation with its own lagged value. To test for this we can use a Ljung Box Test (Wooldridge, 2009).

```{r}
Box.test(snp_complete_joined_noNA_noRec$Return_perc, lag = 20, type = 'Ljung-Box')
```

The p-value of the test, $2.2e^{-16}$, means that we can reject the null hypothesis that there is a unit root, meaning our data is not autocorrelated.

```{r}
acfRes <- acf(x = select(snp_complete_joined_noNA_noRec, c(Date, Return_perc)), lag.max = 3, type = "correlation", plot = TRUE)
```


#### Stationarity
We can use the Augmented Dickey-Fuller Test to see if our data is stationary, or that the "probability distributions are stable over time" (Wooldridge, 2009).

```{r}
adf.test(snp_complete_joined_noNA_noRec$Return_perc)
```

The low p-value of 0.01 or lower means we reject the null hypothesis and accept the alternative hypothesis that the data is stationary.

## Creating Full S&P TS

```{r}
snp_ts_full <- ts(full_dates[, -1], start = c(2006, 361), end = c(2020, 1), frequency = 365)

snp_ts_full <- na.remove(snp_ts_full)


trModel <- lm(Return_perc ~ ., data = snp_ts_full)
summary(trModel)
```

We can see what it looks like with the dummy variables fixed. I choose to remove Wednseday as the literature argues Mondays and Fridays to have the greatest effect and removing Week 53 is fair as there are so few observations of that in the entire dataset (Heyes et al, 2016).
```{r}
full_dates_nowed <- full_dates %>%
  select(-c(Wednesday, week_53))
snp_ts_full_nowed <- ts(full_dates_nowed[, -1], start = c(2006, 361), end = c(2020, 1), frequency = 365)

snp_ts_full_nowed <- na.remove(snp_ts_full_nowed)


trModel_nowed <- lm(Return_perc ~ ., data = snp_ts_full_nowed)
summary(trModel_nowed)
```

Let's try removing the VIX measurement
```{r}
full_dates_novix <- full_dates_nowed %>%
  select(-c(VIX_Close))

snp_ts_full_novix <- ts(full_dates_novix[, -1], start = c(2006, 361), end = c(2020, 1), frequency = 365)

snp_ts_full_novix <- na.remove(snp_ts_full_novix)


trModel_novix <- lm(Return_perc ~ ., data = snp_ts_full_novix)
summary(trModel_novix)
```

Heyes et al postured that increases in PM2.5 would be associated with increases in the VIX, so we would assume that - by removing the VIX measurement - we would see an increase in the significance of PM2.5, but we do not see any statistically significant changes.

### No Wednesday & Using Newey West Test
This is the model ultimately used in the paper!
```{r}
library(lmtest)
library(sandwich)
lmtest::coeftest(trModel_nowed,vcov=NeweyWest(trModel_nowed,verbose=T))
```

### No Vix Newey West
```{r}
snp_ts_full_novix
lmtest::coeftest(trModel_novix,vcov=NeweyWest(trModel_novix,verbose=T))
```



### No Lag Newey West?
```{r}
full_dates_nolag <- full_dates_nowed %>%
  select(-c(Return_perc_lag1, Return_perc_lag2, pm2.5_Daily_Lag1, pm2.5_Daily_Lag2))

snp_ts_full_nolag <- ts(full_dates_nolag[, -1], start = c(2006, 361), end = c(2020, 1), frequency = 365)

snp_ts_full_nolag <- na.remove(snp_ts_full_nolag)


trModel_nolag <- lm(Return_perc ~ ., data = snp_ts_full_nolag)
#summary(trModel_nolag)


lmtest::coeftest(trModel_nolag,vcov=NeweyWest(trModel_nolag,verbose=T))
```


```{r}
plot(trModel_nowed)
```

```{r}
plot(resid(trModel), type="l")
```

## For Paper

### Variables Table

```{r, eval = FALSE, include = FALSE}
library(kableExtra)

vars <- as.character(colnames(snp_complete_joined_noNA_noRec))

vars_df <- as.data.frame(vars) %>%
  mutate(vars = as.character(vars),
         `Variable Type` = "",
         `Units` = "",
         Description = "") %>%
  filter(str_detect(vars, pattern = "week_") == FALSE)

vars_df <- rbind(vars_df, c("week_x", "","", ""))

colnames(vars_df) <- c("Variable", "Variable Type", "Units", "Description")

vars_df <- vars_df %>%
  arrange(Variable) %>%
  mutate(Variable = tolower(Variable))

vars_df <- vars_df %>%
  mutate(`Variable Type` = ifelse(Variable == "monday" | Variable == "tuesday" | Variable == "wedneday" | Variable == "thursday" | Variable == "friday", "Fixed Effects Dummy Variable", `Variable Type`),
         Units = ifelse(Variable == "monday" | Variable == "tuesday" | Variable == "wedneday" | Variable == "thursday" | Variable == "friday", "Binary", Units),
         Description = ifelse(Variable == "monday" | Variable == "tuesday" | Variable == "wedneday" | Variable == "thursday" | Variable == "friday", "A day of the week dummy variable", Description)) %>%
  mutate(`Variable Type` = ifelse(Variable == "co_daily_average", "Averaged 8-Hour Exposure", `Variable Type`),
         Units = ifelse(Variable == "co_daily_average", "PPM", Units),
         Description = ifelse(Variable == "co_daily_average", "The average hourly Carbon Monoxide Measurement", Description))


kable(vars_df, "html") %>%
  kable_styling("striped")

# kable(vars_df, "html") %>%
#   kable_styling("striped") %>%
#   row_spec(1, color = "red") %>%
#   as_image(file = "graphs/test.png")
```


#### Weekdays
```{r}
snp_complete_joined_noNA_noRec %>% 
  select(c(Monday, Tuesday, Wednesday, Thursday, Friday)) %>% 
  summarize_each(funs = sum)
```


```{r}
# snp_complete_joined_noNA_noRec %>% 
#   select(c(Date, Monday, Tuesday, Wednesday, Thursday, Friday)) %>% 
#   ggplot(aes(x = Date)) +
#   geom_smooth(aes(y = cumsum(Monday)), color = "blue", label = "Monday") +
#   geom_smooth(aes(y = cumsum(Tuesday)), color = "red") +
#   geom_smooth(aes(y = cumsum(Wednesday)), color = "green") +
#   geom_smooth(aes(y = cumsum(Thursday)), color = "orange") +
#   geom_smooth(aes(y = cumsum(Friday)), color= "yellow") +
#   theme_fivethirtyeight_mw() +
#   ggtitle("Weekdays") +
#   ylab("Sum of Days") +
#   xlab("Date") +
#   labs(color = "Weekdays") +
#   theme(legend.position = c(0, 1),legend.justification = c(0, 1)) + 
#   scale_color_manual(values = c("blue","red", "green", "orange", "yellow"))

library(dplyr)
weekday_sums <- snp_complete_joined_noNA_noRec %>% 
  select(Date) %>%
  arrange(Date) %>%
  mutate(value = 1) %>%
  mutate(weekdays = factor(weekdays(Date), levels = c("Monday", "Tuesday", "Wednesday","Thursday","Friday"))) %>%
  group_by(weekdays) %>%
  arrange(Date) %>%
  mutate(cv=cumsum(value))

  ggplot(weekday_sums, aes(x = Date, y = cv, color = weekdays))+
  geom_smooth(alpha = 0.5) +
  theme_fivethirtyeight_mw() +
  ggtitle("Weekdays") +
  ylab("Sum of Days") +
  xlab("Date") +
  labs(color = "Weekdays") +
  scale_color_brewer(palette= "Dark2")

ggsave(filename = "graphs/weekdays.png", plot = last_plot(), width = 7, height = 4)
```


### Descriptive Graphs

#### S&P Close Over Time
```{r}
snp_close_gg <- ggplot(snp_complete_noNA_close, aes(x = Date, y = Close)) +
  geom_line() +
  theme_fivethirtyeight_mw() +
  xlab("Date") +
  ylab("S&P Daily Closing Value") +
  ggtitle("Historic S&P 500 Closing Value")

snp_close_gg +
    geom_smooth(method = "lm")

ggsave(filename = "snp_close_plot.png", plot = last_plot(), path = "graphs/", width = 6, height = 4, units = "in", dpi = 300)
```

#### S&P Daily Returns Over Time
```{r}
snp_return_perc <- ggplot(snp_complete_joined_noNA_noRec, aes(x = Date, y = Return_perc)) +
  geom_line() +
  theme_fivethirtyeight_mw() +
  xlab("Date") +
  ylab("S&P Daily Percent Return Value") +
  ggtitle("Historic S&P 500 Daily Percent Return")

snp_return_perc +
    geom_smooth(method = "lm") 

ggsave(filename = "joined_snp_return_perc_plot.png", plot = last_plot(), path = "graphs/", width = 6, height = 4, units = "in", dpi = 300)
```

### VIX 
```{r}
gg_vix <- ggplot(snp_complete_joined_noNA_noRec, aes(x = Date, y = VIX_Close)) +
  geom_line() +
  theme_fivethirtyeight_mw() +
  xlab("Date") +
  ylab("S&P Daily VIX Value") +
  ggtitle("Historic VIX Daily Value")

gg_vix +
    geom_smooth(method = "lm") 

ggsave(filename = "joined_vix_plot.png", plot = last_plot(), path = "graphs/", width = 6, height = 4, units = "in", dpi = 300)
```

### PM2.5

```{r}
gg_pm2.5 <- ggplot(snp_complete_joined_noNA_noRec, aes(x = Date, y = pm2.5_Daily_Sum_UGPM3)) +
  geom_point(size = 0.3, alpha = 0.8) + 
  theme_fivethirtyeight_mw() +
  xlab("Date") +
  ylab("Daily Sum of PM2.5 in ug/meter^3") +
  ggtitle("Daily Division Street PM2.5 Measurements")

gg_pm2.5 +
    geom_smooth(method = "lm", se = FALSE) +
  ylim(0,30)

ggsave(filename = "gg_pm2.5_joined.png", plot = last_plot(), path = "graphs/", width = 6, height = 4, units = "in", dpi = 300)
```

### Ozone
```{r}
library(RColorBrewer)

gg_ozone <- ggplot(snp_complete_joined_noNA_noRec, aes(x = Date, y = ozone_Daily_8hr_Average_PPM)) +
  geom_point(aes(color = factor(`Queens Ozone Data`)), size = 0.3, alpha = 0.8) + 
  theme_fivethirtyeight_mw() +
  xlab("Date") +
  ylab("Daily Sum of Ozone in PPM") +
  ggtitle("Daily Ozone Measurements")

gg_ozone +
    scale_color_manual(values = c("Black", "Red"), name = "Data Source", labels = c("CCNY","Queens College")) +
    geom_smooth(method = "lm", se = FALSE)

ggsave(filename = "gg_ozone_joined.png", plot = last_plot(), path = "graphs/", width = 6, height = 4, units = "in", dpi = 300)
```


### Carbon Monoxide

```{r}
View(snp_complete_joined_noNA_noRec)
```

```{r}
gg_CO <- ggplot(snp_complete_joined_noNA_noRec, aes(x = Date, y = CO_Daily_8hr_Average_PPM)) +
  geom_point(aes(color = factor(`Queens CO Data`)), size = 0.3, alpha = 0.8) + 
  theme_fivethirtyeight_mw() +
  xlab("Date") +
  ylab("Daily Sum of CO in PPM") +
  ggtitle("Daily Carbon Monoxide Measurements")

gg_CO +
  scale_color_manual(values = c("Black", "Red"), name = "Data Source", labels = c("CCNY","Queens College")) +
    geom_smooth(method = "lm", se = FALSE)

ggsave(filename = "gg_CO_joined.png", plot = last_plot(), path = "graphs/", width = 6, height = 4, units = "in", dpi = 300)
```


### Temperature

```{r}
gg_temp <- ggplot(snp_complete_joined_noNA_noRec, aes(x = Date, y = mean_hr_tempF)) +
  geom_point(size = 0.2, alpha = 0.8) + 
  theme_fivethirtyeight_mw() +
  xlab("Date") +
  ylab("Daily Average Temperature (F)") +
  ggtitle("Daily NYC Temperature Trends")

gg_temp +
    geom_smooth(method = "lm", se = FALSE)

ggsave(filename = "gg_temp_joined.png", plot = last_plot(), path = "graphs/", width = 6, height = 4, units = "in", dpi = 300)
```

### Dewpoint

```{r}
gg_dew_temp <- ggplot(snp_complete_joined_noNA_noRec, aes(x = Date, y = mean_hr_dewpoint_tempF)) +
  geom_point(size = 0.2, alpha = 0.8) + 
  theme_fivethirtyeight_mw() +
  xlab("Date") +
  ylab("Daily Average Dewpoint Temperature (F)") +
  ggtitle("Daily NYC Dewpoint Temperature Trends")

gg_dew_temp +
  geom_smooth(method = "lm", se=FALSE)

ggsave(filename = "gg_dew_temp_joined.png", plot = last_plot(), path = "graphs/", width = 6, height = 4, units = "in", dpi = 300)
```
### Air Pressure

```{r}
gg_pressure <- ggplot(snp_complete_joined_noNA_noRec, aes(x = Date, y = mean_hr_pressure_psi)) +
  geom_point(size = 0.2, alpha = 0.8) + 
  theme_fivethirtyeight_mw() +
  xlab("Date") +
  ylab("Daily Average Air Pressure (PSI)") +
  ggtitle("Daily NYC Air Pressure Trends")

gg_pressure +
  geom_smooth(method = "lm", se=FALSE)

ggsave(filename = "gg_pressure_joined.png", plot = last_plot(), path = "graphs/", width = 6, height = 4, units = "in", dpi = 300)
```

### Wind Speed
```{r}
gg_wind <- ggplot(snp_complete_joined_noNA_noRec, aes(x = Date, y = mean_hr_windspeed_knots)) +
  geom_point(size = 0.2, alpha = 0.8) + 
  theme_fivethirtyeight_mw() +
  xlab("Date") +
  ylab("Daily Average Wind Speed (Knots)") +
  ggtitle("Daily NYC Wind Speed Trends")

gg_wind +
  geom_smooth(method = "lm", se=FALSE)

ggsave(filename = "gg_windspeed_joined.png", plot = last_plot(), path = "graphs/", width = 6, height = 4, units = "in", dpi = 300)
```


### Cloud Cover
```{r}
gg_clouds <- ggplot(snp_complete_joined_noNA_noRec, aes(x = Date, y = mean_hr_clouds_percent * 100)) +
  geom_point(size = 0.2, alpha = 0.8) + 
  theme_fivethirtyeight_mw() +
  xlab("Date") +
  ylab("Daily Average Cloud Coverage (%)") +
  ggtitle("Daily NYC Cloud Coverage Trends")

gg_clouds +
  geom_smooth(method = "lm", se=FALSE)

ggsave(filename = "gg_cloudcover_joined.png", plot = last_plot(), path = "graphs/", width = 6, height = 4, units = "in", dpi = 300)
```


### Precipitation 

```{r}
gg_precip <- ggplot(snp_complete_joined_noNA_noRec, aes(x = Date, y = mean_hr_precip_in * 24)) +
  geom_point(size = 0.2, alpha = 0.8) + 
  theme_fivethirtyeight_mw() +
  xlab("Date") +
  ylab("Daily Total Precipitation (inches)") +
  ggtitle("Daily NYC Precipitation Trends")

gg_precip +
  geom_smooth(method = "lm", se=FALSE)

ggsave(filename = "gg_precip_joined.png", plot = last_plot(), path = "graphs/", width = 6, height = 4, units = "in", dpi = 300)
```


## Time Series

How to test some time series:

http://r-statistics.co/Time-Series-Forecasting-With-R.html

#### Seasonality
It is already well documented that the stock market, including the S&P 500, follows seasonal trends (Saunders, 1993; Kamstra, Kramer, Levi 2012). To handle seasonality effects we included dummy variables for each week of the year (Kamstra et al, 2012).

#### Heteroskedasticity
```{r}
model <- lm(Return_perc ~ Date, data = snp_complete_joined_noNA_noRec)

ols_test_breusch_pagan(model)
```

The large Chi^2 (344.9008) indicates that the null hypothesis is false, meaning that variance is not equal - and that the data is, in fact, heteroskedastistic. The original work by Heyes et al overcame similar findings using the Newey West error term, which is used in this as well. However, the Newey West error was not possible to incorporate in the OLS, RandomForest, nor LSTM models. While the machine learning models may adjust for heteroskedasticity to some degree it could be affecting the predictive power of the models overall.

As outlined by Kane, Price, Scotch, and Rabinowitz, ensemble models like RandomForest use “voting” methods that promote higher accuracy aggregate prediction from the constituent trees (2014). These methods were described as some of "the most accurate classification and regression tools currently at the disposal of data scientists" for time series (2014). 

#### Testing for Autocorrelation
First we must do a box test if our model has a "unit root" - which would indicate autocorrelation. Autocorrelation is a problem in time-series analysis, as it means that the data has correlation with its own lagged value. To test for this we can use a Ljung Box Test (Wooldridge, 2009).

```{r}
Box.test(snp_complete_joined_noNA_noRec$Return_perc, lag = 20, type = 'Ljung-Box')
```

The p-value of the test, $2.2e^{-16}$, means that we can reject the null hypothesis that there is a unit root, meaning our data is not autocorrelated.

```{r}
acfRes <- acf(x = select(snp_complete_joined_noNA_noRec, c(Date, Return_perc)), lag.max = 3, type = "correlation", plot = TRUE)
```


#### Stationarity
We can use the Augmented Dickey-Fuller Test to see if our data is stationary, or that the "probability distributions are stable over time" (Wooldridge, 2009).

```{r}
adf.test(snp_complete_joined_noNA_noRec$Return_perc)
```

The low p-value of 0.01 or lower means we reject the null hypothesis and accept the alternative hypothesis that the data is stationary.


## Predictive Models

### Lagged Data
We need to remove all the current values and only use prior day lags for all of our predictive models.

```{r}
lagged_data <- snp_complete_joined_noNA_noRec %>%
  mutate(VIX_lag = lag(VIX_Close, 1)) %>%
  mutate(ozone_lag = lag(ozone_Daily_8hr_Average_PPM, 1)) %>%
  mutate(CO_lag = lag(CO_Daily_8hr_Average_PPM, 1)) %>%
  mutate(Queens_ozone_lag = lag(`Queens Ozone Data`, 1)) %>%
  mutate(Queens_co_lag = lag(`Queens CO Data`, 1)) %>%
  mutate(dewpoint_lag = lag(mean_hr_dewpoint_tempF, 1)) %>%
  mutate(temp_lag = lag(mean_hr_tempF, 1)) %>%
  mutate(pressure_lag = lag(mean_hr_pressure_psi, 1)) %>%
  mutate(wind_lag = lag(mean_hr_windspeed_knots, 1)) %>%
  mutate(precip_lag = lag(mean_hr_precip_in, 1)) %>%
  mutate(cloud_lag = lag(mean_hr_clouds_percent, 1)) %>%
  select(-c(pm2.5_Daily_Sum_UGPM3, VIX_Close, ozone_Daily_8hr_Average_PPM, CO_Daily_8hr_Average_PPM, `Queens Ozone Data`, `Queens CO Data`, mean_hr_dewpoint_tempF, mean_hr_tempF, mean_hr_pressure_psi, mean_hr_windspeed_knots, mean_hr_precip_in, mean_hr_clouds_percent))

lagged_data <- lagged_data[complete.cases(lagged_data),]
```

### Training & Testing

Mike's suggestions: split it 80/20 but keep sequential order! No random splits.

Good quick tips:
https://machinelearningmastery.com/evaluate-machine-learning-algorithms-with-r/

Caret tips for Time Series:
https://topepo.github.io/caret/data-splitting.html#time

In general we will follow three steps:

+ Train/Test split
+ Cross Validation (5 to 10 folds)
+ Repeated Cross Validation: 5 to 10 fold cross validation and 3 or more repeats to give a more robust estimate

### Model Evaluation Definitions
```{r}
library(caret)
seed <- 4132020
ctrl <- trainControl(method="repeatedcv", number=5, repeats=2)
metric <- "Accuracy"
preProcess=c("center", "scale") # I'm not sure if we should center and scale
```

#### Create Sample

Below is wrong, we need to do it sequentially.
```{r}
# set.seed(seed) # Set Seed so that same sample can be reproduced in future also
# # Now Selecting 80% of data as sample from total 'n' rows of the data  
# sample <- sample.int(n = nrow(lagged_data), size = floor(.8*nrow(lagged_data)), replace = F)
# train <- lagged_data[sample, ]
# test  <- lagged_data[-sample, ]
```


##### Sequential Sample

Since we have 2919 rows in the lagged data we can take the first 2334 (just under 80% of the sample) for training, and use the remaining for testing
```{r}
train <- lagged_data[1:2334, ] # lagged_data$Date[2334] == "2017-06-22"

train <- train %>%
  select(-c(week_53, Wednesday))


test <- lagged_data[2335:nrow(lagged_data), ] # lagged_data$Date[2335] == "2017-06-23"

test <- test %>%
  select(-c(week_53, Wednesday))
```

### Lagged OLS

Now make a predictive OLS model with the lagged data??

#### Create Model

```{r}
summary(train$Date)
summary(test$Date)
```


The model now goes from 2006-12-28 to 2017-4-17 in the training dataset and 2017-4-18 to 2020-1-1 in the testing dataset.
```{r}
full_dates_train <- seq(as.Date("2006-12-27"), as.Date("2017-04-17"), by="days")
full_dates_train <- as.data.frame(full_dates_train)
colnames(full_dates_train)  <- "Date"


full_dates_test <- seq(as.Date("2017-04-18"), as.Date("2020-01-01"), by="days")
full_dates_test <- as.data.frame(full_dates_test)
colnames(full_dates_test)  <- "Date"

train_full <- left_join(full_dates_train, train)

test_full <- left_join(full_dates_test, test)

summary(train_full$Date)
summary(test_full$Date)
```


### Evaluating OLS Time Series Model

4/17, the last date in the testing data, is the 107th day of the calendar.
```{r}
lagged_ts <- ts(train_full[, -1], start = c(2006, 361), end = c(2017, 107), frequency = 365)
lagged_ts <- na.remove(lagged_ts)


model <- lm(Return_perc ~ ., data = lagged_ts)
summary(model)
```

The R Squared is extraordinarily low 0.0476 - or about 4.76% goodness of fit. This kind of makes sense in that we know the that the return percentage fluctuates around a mean fit.

#### OLS Prediction

Do we need to do a split train/test here? R2 gives some accuracy but we would for RMSE, in which case how should we go about sampling?

Lagged_ts_test should really start on 4/18/2017, or the 108th day.
```{r}
lagged_ts_test <- ts(test_full[, -1], start = c(2017, 108), end = c(2020, 1), frequency = 365)
lagged_ts_test <- lagged_ts_test[complete.cases(lagged_ts_test),]

lagged_ts_test_df <- as.data.frame(lagged_ts_test)
lagged_ts_test_df <- lagged_ts_test_df[complete.cases(lagged_ts_test_df), ]

prediction <- predict.lm(model, interval= "prediction", newdata = lagged_ts_test_df)

ols_prediction <- as.data.frame(prediction)
summary(ols_prediction)
```

```{r}
RMSE(pred = ols_prediction$fit, obs = lagged_ts_test_df$Return_perc)
```
We calculate a RMSE score of ~0.8034559, which we will use to compare against the other models for goodness of fit.

```{r}
defaultSummary(data.frame(obs = lagged_ts_test_df$Return_perc, pred = ols_prediction$fit))
```


#### Fit

```{r}
lm_pred_fit <- test %>%
  select(c(Date, Return_perc)) %>%
  mutate(predictions = ols_prediction$fit)

lm_pred_fit_long <- lm_pred_fit %>%
  gather("key", "value", -Date)

lm_pred_fit_long <- lm_pred_fit_long %>%
  mutate(key = ifelse(key == "Return_perc", "1. True Return %", key)) %>%
  mutate(key = ifelse(key == "predictions", "2. Predicted Return %", key))

# gg_lm_pred <- ggplot(data = lm_pred_fit, aes(x=Date)) + 
#   geom_line(aes(y=Return_perc), color="Return Percent", alpha = 0.5) +
#   geom_line(color="Predictions", aes(y=predictions)) +
#   theme_fivethirtyeight_mw() +
#   xlab("Date") +
#   ylab("S&P Return Value") +
#   scale_color_manual(values=c("Return Percent"='black',"Predictions"='blue')) +
#   ggtitle("OLS - Actual vs Predicted S&P 500 Return %")

library(plyr)

gg_lm_pred <- ggplot(data = lm_pred_fit_long) + 
    theme_fivethirtyeight_mw() +
  geom_line(aes(x=Date, y = value, color = key), alpha = 1, 
             subset = .(key == '2. Predicted Return %')) +
  geom_line(aes(x=Date, y = value, color = key), alpha = 0.5, 
             subset = .(key == '1. True Return %')) +
  scale_color_manual(values = c("darkgrey", "blue"), labels = c("True Return %", "Predicted Return %")) +
  xlab("Date") +
  ylab("S&P Return Value") +
  ggtitle("OLS - Actual vs Predicted S&P 500 Return %")

ggsave(filename = "actualvspredicted_ols.png", plot = gg_lm_pred, path = "graphs/", width = 6, height = 4, units = "in", dpi = 300)

gg_lm_pred

```


### RandomForest

```{r}
library(doMC)
library(caret)
registerDoMC(parallel::detectCores())
```

```{r, cache=TRUE}
ctrl <- trainControl(method="repeatedcv", number=5, repeats=2)
# this analysis used repeatedcv with 5 cross validations and repeated 2 times
set.seed(seed)

rf_grid <- data.frame(.mtry = 2:(ncol(lagged_ts) - 1L))

out <- train(Return_perc ~ ., data = lagged_ts, method = "rf", trControl = ctrl, tuneGrid = rf_grid, ntrees = 20, importance = TRUE)

varImp(out)

```

rf variable importance

  only 20 most important variables shown (out of 73)
 
                          Overall
pm2.5_Daily_Sum_UGPM3	    100.00000			
dewpoint_lag	            99.26265			
temp_lag	                98.87832			
Return_perc_lag1	        97.91066			
ozone_lag	                91.20418			
pm2.5_Daily_Lag1	        85.19119			
CO_lag	        	        70.78667			
week_51	        	        68.88065			
VIX_lag	        	        67.35894			
pressure_lag	  	        65.76059			
week_7	        	        64.54535			
week_28	    	            62.61524			
pm2.5_Daily_Lag2		      61.62495			
cloud_lag	      	        57.95065			
Return_perc_lag2	        57.07015			
precip_lag	    	        53.49422			
wind_lag	      	        51.54436			
tax_dummy	      	        49.24361			
Queens_co_lag	  	        49.05991			
week_48	        	        46.56240			
20 rows

```{r, cache=TRUE}
library(randomForest)
y_hat <- as.data.frame(predict(out, newdata = lagged_ts_test))

colnames(y_hat) <- "predictions"

lagged_ts_test_df <- as.data.frame(lagged_ts_test)

defaultSummary(data.frame(obs = lagged_ts_test_df$Return_perc, pred = y_hat$predictions))
```

        RMSE    Rsquared         MAE 
0.790097140 0.000146911 0.539590079 


```{r}
rf_comparisons <- test %>%
  select(c(Date, Return_perc)) %>%
  mutate(predictions = y_hat$predictions)


rf_pred_fit_long <- rf_comparisons %>%
  gather("key", "value", -Date)

rf_pred_fit_long <- rf_pred_fit_long %>%
  mutate(key = ifelse(key == "Return_perc", "1. True Return %", key)) %>%
  mutate(key = ifelse(key == "predictions", "2. Predicted Return %", key))


library(plyr)

gg_rf <- ggplot(rf_pred_fit_long) +
  theme_fivethirtyeight_mw() +
  geom_line(aes(x=Date, y = value, color = key), alpha = 1, 
             subset = .(key == '2. Predicted Return %')) +
  geom_line(aes(x=Date, y = value, color = key), alpha = 0.5, 
             subset = .(key == '1. True Return %')) +
  scale_color_manual(values = c("darkgrey", "blue"), labels = c("True Return %", "Predicted Return %")) +
  xlab("Date") +
  ylab("S&P Return Value") +
  ggtitle("RF - Actual vs Predicted S&P 500 Return %")
  
ggsave(filename = "actualvspredicted_rf.png", plot = gg_rf, path = "graphs/", width = 6, height = 4, units = "in", dpi = 300)

gg_rf
```

#### Gather test

```{r}
rf_gathered <- gather(rf_comparisons, "source", "value", -Date)
#head(rf_gathered)

gg_rf <- rf_gathered %>%
  ggplot(aes(x=Date, y = value)) +
  geom_line(aes(color=source)) +
  scale_color_discrete(labels = c("Predicted Return %", "Actual Return %")) +
  theme_fivethirtyeight_mw() +
  xlab("Date") +
  ylab("S&P Return Value") +
  ggtitle("Actual vs Predicted S&P 500 Return Percent") +
  aes(group=rev(source))

ggsave(filename = "actualvspredicted_rf_color.png", plot = gg_rf, path = "graphs/", width = 6, height = 4, units = "in", dpi = 300)

gg_rf
```


### LSTM

I'm using this page as reference:

https://www.r-bloggers.com/time-series-deep-learning-forecasting-sunspots-with-keras-stateful-lstm-in-r/

#### Preprocess Data

We should remove the second an third lagged Return_Perc variables because you don't need them in LSTM.

We'll leave in the one day lags because those are what we're using for all of the variables.

```{r}
train_keras <- train %>%
  mutate(key = "Training")

test_keras <- test %>%
  mutate(key = "Testing")

full_keras <- rbind(train_keras, test_keras)

full_keras <- full_keras %>%
  rename(index = Date) %>%
  rename(value = Return_perc)

write_csv(x = full_keras, path = "LSTM/keras_training_testing.csv", col_names = TRUE)
```


#### LSTM R Packages

```{r}
set.seed(seed)
# Core Tidyverse
library(tidyverse)
library(glue)
library(forcats)

# Time Series
library(timetk)
library(tidyquant)
library(tibbletime)

# Visualization
library(cowplot)

# Preprocessing
library(recipes)

# Sampling / Accuracy
library(rsample)
library(yardstick) 

# Modeling
library(keras)
```


#### Check ACF

We need to check the full ACF for auto correlation do determine where to draw the lags - though it may be best to keep this to 2-3 days of lag to make it relatively similar to the other models.

```{r}
tidy_acf <- function(data, value, lags = 0:20) {
    
    value_expr <- enquo(value)
    
    acf_values <- data %>%
        pull(value) %>%
        acf(lag.max = tail(lags, 1), plot = FALSE) %>%
        .$acf %>%
        .[,,1]
    
    ret <- tibble(acf = acf_values) %>%
        rowid_to_column(var = "lag") %>%
        mutate(lag = lag - 1) %>%
        filter(lag %in% lags)
    
    return(ret)
}

max_lag <- nrow(full_keras) - 1

full_keras %>%
    tidy_acf(value, lags = 0:max_lag) %>%
  head()
```

```{r}
full_keras %>%
    tidy_acf(value, lags = 0:max_lag) %>%
    ggplot(aes(lag, acf)) +
    geom_segment(aes(xend = lag, yend = 0)) +
    theme_fivethirtyeight_mw() +
    labs(title = "ACF: S&P 500 Return")
```

```{r}
full_keras %>%
    tidy_acf(value, lags = 0:max_lag) %>%
    ggplot(aes(lag, acf)) +
    geom_segment(aes(xend = lag, yend = 0)) +
    xlim(0, 20) +
    theme_fivethirtyeight_mw() +
    labs(title = "ACF: S&P 500 Return")
```

There's virtually no autocorrelation beyond the first, so we might as well set it to 3 days of lagged values to be consistent with the other models.

NOTE: The rest of the LSTM model was done in Pythong

##### LSTM Graph
I am reimporting the LSTM model into R for graphing
```{r}
lstm_true = read.csv("./LSTM/lstm_true_values.csv")
lstm_true = lstm_true %>%
  select(X0)

colnames(lstm_true) <- "Return_perc"

lstm_pred = read.csv("./LSTM/lstm_predicted_values.csv")
lstm_pred = lstm_pred %>%
  select(X0)

colnames(lstm_pred) <- "predictions"

lstm_full <- cbind(lstm_true, lstm_pred)

lstm_full$Date <- rf_comparisons$Date[39:630]
```


```{r}
head(lstm_full)
nrow(lstm_full)
```

```{r}
lstm_full_long <- lstm_full %>%
  gather("key", "value", -Date)


lstm_full_long <- lstm_full_long %>%
  mutate(key = ifelse(key == "Return_perc", "1. True Return %", key)) %>%
  mutate(key = ifelse(key == "predictions", "2. Predicted Return %", key))


library(plyr)

gg_lstm <- ggplot(lstm_full_long) +
  theme_fivethirtyeight_mw() +
  geom_line(aes(x=Date, y = value, color = key), alpha = 1, 
             subset = .(key == '2. Predicted Return %')) +
  geom_line(aes(x=Date, y = value, color = key), alpha = 0.5, 
             subset = .(key == '1. True Return %')) +
  scale_color_manual(values = c("darkgrey", "blue"), labels = c("True Return %", "Predicted Return %")) +
  xlab("Date") +
  ylab("Normalized S&P Return Value") +
  ggtitle("LSTM - Actual vs Predicted S&P 500 Return %")
gg_lstm 

ggsave(filename = "actualvspredicted_lstm.png", plot = gg_lstm, path = "graphs/", width = 6.5, height = 4, units = "in", dpi = 300)
```


