---
title: "Inter Intra Joined data Time Series Tests"
author: "Michael Weisner"
date: "7/15/2020"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE,warning=FALSE)
library(tinytex)
library(kableExtra)
library(qwraps2)
library(stringr)
library(httr)
library(data.table)
library(lubridate)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(ggthemes)

library(tseries)
library(gridExtra)
library(forecast)
library(olsrr)

library(caret)
library(doMC)

library(lmtest)
library(sandwich)

library(kableExtra)


options(qwraps2_markup = "markdown")
```

```{r}
theme_fivethirtyeight_mw <- function(base_size = 12, base_family = "sans") {
  colors <- deframe(ggthemes::ggthemes_data[["fivethirtyeight"]])
  (theme_foundation(base_size = base_size, base_family = base_family)
   + theme(
     line = element_line(colour = "black"),
     rect = element_rect(fill = colors["Light Gray"],
                         linetype = 0, colour = NA),
     text = element_text(colour = colors["Dark Gray"]),
     axis.ticks = element_blank(),
     axis.line = element_blank(),
     legend.background = element_rect(),
     legend.position = "bottom",
     legend.direction = "horizontal",
     legend.box = "vertical",
     panel.grid = element_line(colour = NULL),
     panel.grid.major =
       element_line(colour = colors["Medium Gray"]),
     panel.grid.minor = element_blank(),
     # unfortunately, can't mimic subtitles TODO!
     plot.title = element_text(hjust = 0, size = rel(1.5), face = "bold"),
     plot.margin = unit(c(1, 1, 1, 1), "lines"),
     strip.background = element_rect()))
}
```

## Data

```{r}
snp_complete_joined_noNA_noRec <- read_csv("data_inter_intra/JOINED_snp_complete_noNA_noRec_noclose_inter_intra.csv")

snp_complete_joined_noNA_noRec_intraday <- snp_complete_joined_noNA_noRec %>%
  select(-c(Return_perc_interday, Return_perc_interday_lag1, Return_perc_interday_lag2))
  
snp_complete_joined_noNA_noRec_interday <- snp_complete_joined_noNA_noRec %>%
  select(-c(Return_perc_intraday, Return_perc_intraday_lag1, Return_perc_intraday_lag2))

snp_complete_noNA_close <- read_csv("data_inter_intra/JOINED_snp_complete_noNA_noRec_close_inter_intra.csv")
```

## Summary Statistics

```{r}
snp_no_dummies <- snp_complete_joined_noNA_noRec %>%
  select(c(Return_perc_interday, Return_perc_interday_lag1, Return_perc_interday_lag2, Return_perc_intraday, Return_perc_intraday_lag1, Return_perc_intraday_lag2, VIX_Close, pm2.5_Daily_Sum_UGPM3, ozone_Daily_8hr_Average_PPM, CO_Daily_8hr_Average_PPM, mean_hr_dewpoint_tempF, mean_hr_tempF, mean_hr_pressure_psi, mean_hr_windspeed_knots, mean_hr_precip_in, mean_hr_clouds_percent))

variable_descriptive_stats <- psych::describe(snp_no_dummies)

variable_descriptive_stats <- variable_descriptive_stats %>%
  select(-c(vars, trimmed, mad, skew, kurtosis))

rownames(variable_descriptive_stats) <- c("Interday Return Percentage", "Interday Return Percentage 1 Day Lag", "Interday Return Percentage 2 Day Lag", "Intraday Return Percentage", "Intraday Return Percentage 1 Day Lag", "Intraday Return Percentage 2 Day Lag", "VIX Closing", "PM2.5 Daily Sum (ug per cubic meter)", "Ozone Daily 8 Hour Average (PPM)", "CO Daily 8 Hour Average (PPM)", "Mean Hourly Dewpoint (F)", "Mean Hourly Temperature (F)", "Mean Hourly Pressure (PSI)", "Mean Hourly Windspeed (Knots)", "Mean Hourly Precipitation (inches)", "Mean Hourly Cloud Coverage (Percentage)")

variable_descriptive_stats %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped","condensed"))
```



## Test with ts()

```{r}
#full_dates <- seq(as.Date("2008-07-10"), as.Date("2020-01-01"), by="days")

full_dates <- seq(as.Date("2006-12-27"), as.Date("2020-01-01"), by="days")
full_dates <- as.data.frame(full_dates)
colnames(full_dates)  <- "Date"

# 2019-11-29 is the current full amount of days

# snp_ols <- snp_complete_joined_noNA_noRec %>%
#   mutate(Close_lag1 = lag(x = snp_complete_joined_noNA_noRec$Close, n = 1L)) 
# 
# snp_ols <- snp_ols %>%
#   mutate(Close_lag2 = lag(x = snp_ols$Close_lag1, n = 1L))

snp_ols <- snp_complete_joined_noNA_noRec
snp_ols_interday <- snp_complete_joined_noNA_noRec_interday
snp_ols_intraday <- snp_complete_joined_noNA_noRec_intraday


 
full_dates_interday <- left_join(full_dates, snp_ols_interday)
full_dates_intraday <- left_join(full_dates, snp_ols_intraday)

full_dates <-left_join(full_dates, snp_ols)
```

### Remove Weekends from full_dates
```{r}
full_dates_weeks <- full_dates %>%
  mutate(weekdays = weekdays(full_dates$Date)) %>%
  filter(weekdays != "Saturday" & weekdays != "Sunday")

incompletes <- full_dates_weeks[!complete.cases(full_dates_weeks), ]
completes <- full_dates_weeks[complete.cases(full_dates_weeks), ]

for_keras_interday <- completes %>%
  select(-c(weekdays, Return_perc_intraday, Return_perc_interday_lag1, Return_perc_interday_lag2, Return_perc_intraday_lag1, Return_perc_intraday_lag2, pm2.5_Daily_Lag2)) %>%
  rename(index = Date) %>%
  rename(value = Return_perc_interday)

for_keras_intraday <- completes %>%
  select(-c(weekdays, Return_perc_interday, Return_perc_interday_lag1, Return_perc_interday_lag2, Return_perc_intraday_lag1, Return_perc_intraday_lag2, pm2.5_Daily_Lag1, pm2.5_Daily_Lag2)) %>%
  rename(index = Date) %>%
  rename(value = Return_perc_intraday)

write_csv(x = for_keras_interday, path = "LSTM/keras_nolag_completes_interday.csv")

write_csv(x = for_keras_intraday, path = "LSTM/keras_nolag_completes_intraday.csv")

nrow(completes)
nrow(incompletes)
```
Out of 3396 total weekdays (excluding the financial crisis of 2008) we have 2965 complete cases and 431 missing cases, or about 14% missing days. These may be somewhat accounted for by federal holidays.

### Quick Comparison of Interday and Intraday

```{r}
comparison <- completes %>%
  select(c(Date, Return_perc_interday, Return_perc_intraday))

gg_compare_interday <- ggplot(comparison, aes(x=Date, y=Return_perc_interday)) +
  geom_line(color = "black") +
  geom_smooth(se=FALSE)+
  theme_fivethirtyeight_mw()

gg_compare_interday
```

```{r}
gg_compare_intraday <- ggplot(comparison, aes(x=Date, y=Return_perc_intraday)) +
  geom_line(color = "black") +
  theme_fivethirtyeight_mw()


gg_compare_intraday
```

```{r}
gg_compare_overlay <- ggplot(comparison, aes(x=Date)) +
  geom_line(aes(y=Return_perc_intraday), color = "blue", alpha=0.4) +
  geom_point(aes(y=Return_perc_intraday), size = 2, color = "blue", alpha=0.3)+
  geom_line(aes(y=Return_perc_interday), color = "red", alpha=0.2) +
  geom_point(aes(y=Return_perc_interday), size = 2, color = "red", alpha=0.3)+
  theme_fivethirtyeight_mw()


gg_compare_overlay
```

```{r}
gg_compare_diff <- ggplot(comparison, aes(x=Date, y=Return_perc_interday-Return_perc_intraday)) +
  geom_line(color = "black") +
  theme_fivethirtyeight_mw() +
  scale_y_continuous(breaks=c(-8,-4,0,4), limits=c(-10,5))

gg_compare_diff
```



## S&P 500 Returns Intraday Time Series
In order to create the original model, we will use intraday return percentages to calculate the value.
```{r}
simple_intraday <- select(snp_complete_joined_noNA_noRec, c("Date", "Return_perc_intraday"))
simple_interday <- select(snp_complete_joined_noNA_noRec, c("Date", "Return_perc_interday"))

simple_ts_intraday <- ts(simple_intraday)
simple_ts_interday <- ts(simple_interday)
```


### Decomposition
```{r}
snp_ts_interday <- ts(full_dates_interday[, 2], start = c(2006, 361), end = c(2020, 1), frequency = 365)
snp_ts_intraday <- ts(full_dates_intraday[, 2], start = c(2006, 361), end = c(2020, 1), frequency = 365)

# 2019-11-29 is the last day in the sample annoyingly

snp_ts_interday <- na.remove(snp_ts_interday)
snp_ts_intraday <- na.remove(snp_ts_intraday)


# Decompose
decomposedRes_interday <- decompose(snp_ts_interday, type="additive")
decomposedRes_intraday <- decompose(snp_ts_intraday, type="additive")


plot(decomposedRes_interday)
plot(decomposedRes_intraday)
```


### ACF & PACF
```{r}
acfRes_interday <- acf(snp_ts_interday, type = "correlation")
acfRes_intraday <- acf(snp_ts_intraday, type = "correlation")
```

```{r}
pacfRes_interday <- pacf(snp_ts_interday)
pacfRes_intraday <- pacf(snp_ts_intraday)

```

#### Full ACF Test
```{r}
tidy_acf <- function(data, value, lags = 0:20) {
    
    value_expr <- enquo(value)
    
    acf_values <- data %>%
        pull(value) %>%
        acf(lag.max = tail(lags, 1), plot = FALSE) %>%
        .$acf %>%
        .[,,1]
    
    ret <- tibble(acf = acf_values) %>%
        rowid_to_column(var = "lag") %>%
        mutate(lag = lag - 1) %>%
        filter(lag %in% lags)
    
    return(ret)
}

max_lag <- nrow(snp_complete_joined_noNA_noRec) - 1


snp_complete_joined_noNA_noRec %>%
    tidy_acf("Return_perc_interday", lags = 0:max_lag) %>%
    ggplot(aes(lag, acf)) +
    geom_segment(aes(xend = lag, yend = 0)) +
    theme_fivethirtyeight_mw() +
    labs(title = "ACF: S&P 500 Interday Return")

snp_complete_joined_noNA_noRec %>%
    tidy_acf("Return_perc_intraday", lags = 0:max_lag) %>%
    ggplot(aes(lag, acf)) +
    geom_segment(aes(xend = lag, yend = 0)) +
    theme_fivethirtyeight_mw() +
    labs(title = "ACF: S&P 500 Intraday Return")
```

#### Seasonality
It is already well documented that the stock market, including the S&P 500, follows seasonal trends (Saunders, 1993; Kamstra, Kramer, Levi 2012). To handle seasonality effects we included dummy variables for each week of the year (Kamstra et al, 2012).

#### Heteroskedasticity

Interday
```{r}
interday_model <- lm(Return_perc_interday ~ Date, data = snp_complete_joined_noNA_noRec_interday)

ols_test_breusch_pagan(interday_model)
```


Intraday
```{r}
intraday_model <- lm(Return_perc_intraday ~ Date, data = snp_complete_joined_noNA_noRec_intraday)

ols_test_breusch_pagan(intraday_model)
```


#### Testing for Autocorrelation
First we must do a box test if our model has a "unit root" - which would indicate autocorrelation. Autocorrelation is a problem in time-series analysis, as it means that the data has correlation with its own lagged value. To test for this we can use a Ljung Box Test (Wooldridge, 2009).

Interday
```{r}
Box.test(snp_complete_joined_noNA_noRec_interday$Return_perc_interday, lag = 20, type = 'Ljung-Box')
```
Intraday
```{r}
Box.test(snp_complete_joined_noNA_noRec_intraday$Return_perc_intraday, lag = 20, type = 'Ljung-Box')
```


The p-value of the test, $2.995e^{-08}$, means that we can reject the null hypothesis that there is a unit root, meaning our data is not autocorrelated.

Interday
```{r}
acfRes_interday <- acf(x = select(snp_complete_joined_noNA_noRec_interday, c(Date, Return_perc_interday)), lag.max = 3, type = "correlation", plot = TRUE)
```

Intraday
```{r}
acfRes_intraday <- acf(x = select(snp_complete_joined_noNA_noRec_intraday, c(Date, Return_perc_intraday)), lag.max = 3, type = "correlation", plot = TRUE)

```


#### Stationarity
We can use the Augmented Dickey-Fuller Test to see if our data is stationary, or that the "probability distributions are stable over time" (Wooldridge, 2009).

Interday
```{r}
adf.test(snp_complete_joined_noNA_noRec_interday$Return_perc_interday)
```

Intraday
```{r}
adf.test(snp_complete_joined_noNA_noRec_intraday$Return_perc_intraday)
```

The low p-value of 0.01 or lower means we reject the null hypothesis and accept the alternative hypothesis that the data is stationary.

## Creating Full S&P TS

Interday
```{r}
snp_ts_interday_full <- ts(select(full_dates, -c(Date, Return_perc_intraday, Return_perc_intraday_lag1, Return_perc_intraday_lag2)), start = c(2006, 361), end = c(2020, 1), frequency = 365)

snp_ts_interday_full <- na.remove(snp_ts_interday_full)


trModel <- lm(Return_perc_interday ~ ., data = snp_ts_interday_full)
summary(trModel)
```

Intraday
```{r}
snp_ts_intraday_full <- ts(select(full_dates, -c(Date, Return_perc_interday, Return_perc_interday_lag1, Return_perc_interday_lag2)), start = c(2006, 361), end = c(2020, 1), frequency = 365)

snp_ts_intraday_full <- na.remove(snp_ts_intraday_full)


trModel <- lm(Return_perc_intraday ~ ., data = snp_ts_intraday_full)
summary(trModel)
```


We can see what it looks like with the dummy variables fixed. I choose to remove Wednseday as the literature argues Mondays and Fridays to have the greatest effect and removing Week 53 is fair as there are so few observations of that in the entire dataset (Heyes et al, 2016).

Interday
```{r}
full_dates_interday_nowed <- full_dates %>%
  select(-c(Date, Return_perc_intraday, Return_perc_intraday_lag1, Return_perc_intraday_lag2, Wednesday, week_53))
full_dates_interday_nowed <- ts(full_dates_interday_nowed, start = c(2006, 361), end = c(2020, 1), frequency = 365)

full_dates_interday_nowed <- na.remove(full_dates_interday_nowed)


trModel_interday_nowed <- lm(Return_perc_interday ~ ., data = full_dates_interday_nowed)
summary(trModel_interday_nowed)
```


Intraday
```{r}
full_dates_intraday_nowed <- full_dates %>%
  select(-c(Date, Return_perc_interday, Return_perc_interday_lag1, Return_perc_interday_lag2, Wednesday, week_53))


full_dates_intraday_nowed <- ts(full_dates_intraday_nowed, start = c(2006, 361), end = c(2020, 1), frequency = 365)

full_dates_intraday_nowed <- na.remove(full_dates_intraday_nowed)


trModel_intraday_nowed <- lm(Return_perc_intraday ~ ., data = full_dates_intraday_nowed)
summary(trModel_intraday_nowed)
```


Let's try removing the VIX measurement
Interday
```{r}
full_dates_interday_novix <- full_dates %>%
  select(-c(Date, VIX_Close, Return_perc_intraday, Return_perc_intraday_lag1, Return_perc_intraday_lag2, Wednesday, week_53))

snp_ts_full_interday_novix <- ts(full_dates_interday_novix, start = c(2006, 361), end = c(2020, 1), frequency = 365)

snp_ts_full_interday_novix <- na.remove(snp_ts_full_interday_novix)


trModel_interday_novix <- lm(Return_perc_interday ~ ., data = snp_ts_full_interday_novix)
summary(trModel_interday_novix)
```

Intraday
```{r}
full_dates_intraday_novix <- full_dates %>%
  select(-c(Date, VIX_Close, Return_perc_interday, Return_perc_interday_lag1, Return_perc_interday_lag2, Wednesday, week_53))

snp_ts_full_intraday_novix <- ts(full_dates_intraday_novix, start = c(2006, 361), end = c(2020, 1), frequency = 365)

snp_ts_full_intraday_novix <- na.remove(snp_ts_full_intraday_novix)


trModel_intraday_novix <- lm(Return_perc_intraday ~ ., data = snp_ts_full_intraday_novix)
summary(trModel_intraday_novix)
```

Heyes et al postured that increases in PM2.5 would be associated with increases in the VIX, so we would assume that - by removing the VIX measurement - we would see an increase in the significance of PM2.5, but we do not see any statistically significant changes.

### No Wednesday & Using Newey West Test
This is the model ultimately used in the paper!
Interday
```{r}
library(lmtest)
library(sandwich)
lmtest::coeftest(trModel_interday_nowed,vcov=NeweyWest(trModel_interday_nowed,verbose=T))
```

Intraday
```{r}
library(lmtest)
library(sandwich)
lmtest::coeftest(trModel_intraday_nowed,vcov=NeweyWest(trModel_intraday_nowed,verbose=T))
```

### No Vix Newey West
Interday
```{r}
#snp_ts_full_interday_novix
lmtest::coeftest(trModel_interday_novix,vcov=NeweyWest(trModel_interday_novix,verbose=T))
```

Intraday
```{r}
#snp_ts_full_intraday_novix
lmtest::coeftest(trModel_intraday_novix,vcov=NeweyWest(trModel_intraday_novix,verbose=T))
```

### No Lag Newey West?

Interday
```{r}
full_dates_interday_nolag <- full_dates %>%
  select(-c(Date, Return_perc_intraday, Return_perc_intraday_lag1, Return_perc_intraday_lag2, Wednesday, week_53)) %>%
  select(-c(Return_perc_interday_lag1, Return_perc_interday_lag2, pm2.5_Daily_Lag1, pm2.5_Daily_Lag2))

snp_ts_full_interday_nolag <- ts(full_dates_interday_nolag, start = c(2006, 361), end = c(2020, 1), frequency = 365)

snp_ts_full_interday_nolag <- na.remove(snp_ts_full_interday_nolag)


trModel_interday_nolag <- lm(Return_perc_interday ~ ., data = snp_ts_full_interday_nolag)
#summary(trModel_interday_nolag)


lmtest::coeftest(trModel_interday_nolag,vcov=NeweyWest(trModel_interday_nolag,verbose=T))
```

Intraday
```{r}
full_dates_intraday_nolag <- full_dates %>%
  select(-c(Date, Return_perc_interday, Return_perc_interday_lag1, Return_perc_interday_lag2, Wednesday, week_53)) %>%
  select(-c(Return_perc_intraday_lag1, Return_perc_intraday_lag2, pm2.5_Daily_Lag1, pm2.5_Daily_Lag2))

snp_ts_full_intraday_nolag <- ts(full_dates_intraday_nolag, start = c(2006, 361), end = c(2020, 1), frequency = 365)

snp_ts_full_intraday_nolag <- na.remove(snp_ts_full_intraday_nolag)


trModel_intraday_nolag <- lm(Return_perc_intraday ~ ., data = snp_ts_full_intraday_nolag)
#summary(trModel_intraday_nolag)


lmtest::coeftest(trModel_intraday_nolag,vcov=NeweyWest(trModel_intraday_nolag,verbose=T))
```


### Model Plots
Interday
```{r}
plot(trModel_interday_nowed)
```

```{r}
plot(resid(trModel_interday_nowed), type="l")
```


Intraday
```{r}
plot(trModel_intraday_nowed)
```

```{r}
plot(resid(trModel_intraday_nowed), type="l")
```



## Tables and Graphs For Paper

### Variables Table

```{r, eval = FALSE, include = FALSE}
library(kableExtra)

vars <- as.character(colnames(snp_complete_joined_noNA_noRec))

vars_df <- as.data.frame(vars) %>%
  mutate(vars = as.character(vars),
         `Variable Type` = "",
         `Units` = "",
         Description = "") %>%
  filter(str_detect(vars, pattern = "week_") == FALSE)

vars_df <- rbind(vars_df, c("week_x", "","", ""))

colnames(vars_df) <- c("Variable", "Variable Type", "Units", "Description")

vars_df <- vars_df %>%
  arrange(Variable) %>%
  mutate(Variable = tolower(Variable))

vars_df <- vars_df %>%
  mutate(`Variable Type` = ifelse(Variable == "monday" | Variable == "tuesday" | Variable == "wedneday" | Variable == "thursday" | Variable == "friday", "Fixed Effects Dummy Variable", `Variable Type`),
         Units = ifelse(Variable == "monday" | Variable == "tuesday" | Variable == "wedneday" | Variable == "thursday" | Variable == "friday", "Binary", Units),
         Description = ifelse(Variable == "monday" | Variable == "tuesday" | Variable == "wedneday" | Variable == "thursday" | Variable == "friday", "A day of the week dummy variable", Description)) %>%
  mutate(`Variable Type` = ifelse(Variable == "co_daily_average", "Averaged 8-Hour Exposure", `Variable Type`),
         Units = ifelse(Variable == "co_daily_average", "PPM", Units),
         Description = ifelse(Variable == "co_daily_average", "The average hourly Carbon Monoxide Measurement", Description))


kable(vars_df, "html") %>%
  kable_styling("striped")

# kable(vars_df, "html") %>%
#   kable_styling("striped") %>%
#   row_spec(1, color = "red") %>%
#   as_image(file = "graphs/test.png")
```


#### Weekdays
```{r}
snp_complete_joined_noNA_noRec %>% 
  select(c(Monday, Tuesday, Wednesday, Thursday, Friday)) %>% 
  summarize_each(funs = sum)
```


```{r}
# snp_complete_joined_noNA_noRec %>% 
#   select(c(Date, Monday, Tuesday, Wednesday, Thursday, Friday)) %>% 
#   ggplot(aes(x = Date)) +
#   geom_smooth(aes(y = cumsum(Monday)), color = "blue", label = "Monday") +
#   geom_smooth(aes(y = cumsum(Tuesday)), color = "red") +
#   geom_smooth(aes(y = cumsum(Wednesday)), color = "green") +
#   geom_smooth(aes(y = cumsum(Thursday)), color = "orange") +
#   geom_smooth(aes(y = cumsum(Friday)), color= "yellow") +
#   theme_fivethirtyeight_mw() +
#   ggtitle("Weekdays") +
#   ylab("Sum of Days") +
#   xlab("Date") +
#   labs(color = "Weekdays") +
#   theme(legend.position = c(0, 1),legend.justification = c(0, 1)) + 
#   scale_color_manual(values = c("blue","red", "green", "orange", "yellow"))

library(dplyr)
weekday_sums <- snp_complete_joined_noNA_noRec %>% 
  select(Date) %>%
  arrange(Date) %>%
  mutate(value = 1) %>%
  mutate(weekdays = factor(weekdays(Date), levels = c("Monday", "Tuesday", "Wednesday","Thursday","Friday"))) %>%
  group_by(weekdays) %>%
  arrange(Date) %>%
  mutate(cv=cumsum(value))

  ggplot(weekday_sums, aes(x = Date, y = cv, color = weekdays))+
  geom_smooth(alpha = 0.5) +
  theme_fivethirtyeight_mw() +
  ggtitle("Weekdays") +
  ylab("Sum of Days") +
  xlab("Date") +
  labs(color = "Weekdays") +
  scale_color_brewer(palette= "Dark2")

ggsave(filename = "graphs/weekdays.png", plot = last_plot(), width = 7, height = 4)
```



### Descriptive Graphs

#### S&P Close Over Time
```{r}
snp_close_gg <- ggplot(snp_complete_noNA_close, aes(x = Date, y = Close)) +
  geom_line() +
  theme_fivethirtyeight_mw() +
  xlab("Date") +
  ylab("S&P Daily Closing Value") +
  ggtitle("Historic S&P 500 Closing Value")

snp_close_gg +
    geom_smooth(method = "lm")

ggsave(filename = "snp_close_plot.png", plot = last_plot(), path = "graphs/", width = 6, height = 4, units = "in", dpi = 300)
```

#### S&P Daily Returns Over Time
```{r}
snp_return_perc <- ggplot(snp_complete_joined_noNA_noRec_interday, aes(x = Date, y = Return_perc_interday)) +
  geom_line() +
  theme_fivethirtyeight_mw() +
  xlab("Date") +
  ylab("S&P Daily Percent Return Value") +
  ggtitle("Historic S&P 500 Daily Interday % Return")

snp_return_perc +
    geom_smooth(method = "lm") 

ggsave(filename = "joined_snp_return_perc_interday_plot.png", plot = last_plot(), path = "graphs/", width = 6, height = 4, units = "in", dpi = 300)
```

```{r}
snp_return_perc <- ggplot(snp_complete_joined_noNA_noRec_intraday, aes(x = Date, y = Return_perc_intraday)) +
  geom_line() +
  theme_fivethirtyeight_mw() +
  xlab("Date") +
  ylab("S&P Daily Percent Return Value") +
  ggtitle("Historic S&P 500 Daily Intraday % Return")

snp_return_perc +
    geom_smooth(method = "lm") 

ggsave(filename = "joined_snp_return_perc_intraday_plot.png", plot = last_plot(), path = "graphs/", width = 6, height = 4, units = "in", dpi = 300)
```


### VIX 
```{r}
gg_vix <- ggplot(snp_complete_joined_noNA_noRec_interday, aes(x = Date, y = VIX_Close)) +
  geom_line() +
  theme_fivethirtyeight_mw() +
  xlab("Date") +
  ylab("S&P Daily VIX Value") +
  ggtitle("Historic VIX Daily Value")

gg_vix +
    geom_smooth(method = "lm") 

ggsave(filename = "joined_vix_plot.png", plot = last_plot(), path = "graphs/", width = 6, height = 4, units = "in", dpi = 300)
```

### PM2.5

```{r}
gg_pm2.5 <- ggplot(snp_complete_joined_noNA_noRec_interday, aes(x = Date, y = pm2.5_Daily_Sum_UGPM3)) +
  geom_point(size = 0.3, alpha = 0.8) + 
  geom_smooth(method = "lm", se = FALSE) +
  theme_fivethirtyeight_mw() +
  xlab("Date") +
  ylab("Daily Sum of PM2.5 in ug/meter^3") +
  ggtitle("Daily Division Street PM2.5 Measurements")

ggsave(filename = "gg_pm2.5_joined.png", plot = last_plot(), path = "graphs/", width = 6, height = 4, units = "in", dpi = 300)
```

### Ozone
```{r}
library(RColorBrewer)

gg_ozone <- ggplot(snp_complete_joined_noNA_noRec_interday, aes(x = Date, y = ozone_Daily_8hr_Average_PPM)) +
  geom_point(aes(color = factor(`Queens Ozone Data`)), size = 0.3, alpha = 0.8) + 
  theme_fivethirtyeight_mw() +
  xlab("Date") +
  ylab("Daily Sum of Ozone in PPM") +
  ggtitle("Daily Ozone Measurements")

gg_ozone +
    scale_color_manual(values = c("Black", "Red"), name = "Data Source", labels = c("CCNY","Queens College")) +
    geom_smooth(method = "lm", se = FALSE)

ggsave(filename = "gg_ozone_joined.png", plot = last_plot(), path = "graphs/", width = 6, height = 4, units = "in", dpi = 300)
```


### Carbon Monoxide

```{r}
#View(snp_complete_joined_noNA_noRec)
```

```{r}
gg_CO <- ggplot(snp_complete_joined_noNA_noRec_interday, aes(x = Date, y = CO_Daily_8hr_Average_PPM)) +
  geom_point(aes(color = factor(`Queens CO Data`)), size = 0.3, alpha = 0.8) + 
  theme_fivethirtyeight_mw() +
  xlab("Date") +
  ylab("Daily Sum of CO in PPM") +
  ggtitle("Daily Carbon Monoxide Measurements")

gg_CO +
  scale_color_manual(values = c("Black", "Red"), name = "Data Source", labels = c("CCNY","Queens College")) +
    geom_smooth(method = "lm", se = FALSE)

ggsave(filename = "gg_CO_joined.png", plot = last_plot(), path = "graphs/", width = 6, height = 4, units = "in", dpi = 300)
```


### Temperature

```{r}
gg_temp <- ggplot(snp_complete_joined_noNA_noRec_interday, aes(x = Date, y = mean_hr_tempF)) +
  geom_point(size = 0.2, alpha = 0.8) + 
  theme_fivethirtyeight_mw() +
  xlab("Date") +
  ylab("Daily Average Temperature (F)") +
  ggtitle("Daily NYC Temperature Trends")

gg_temp +
    geom_smooth(method = "lm", se = FALSE)

ggsave(filename = "gg_temp_joined.png", plot = last_plot(), path = "graphs/", width = 6, height = 4, units = "in", dpi = 300)
```

### Dewpoint

```{r}
gg_dew_temp <- ggplot(snp_complete_joined_noNA_noRec_interday, aes(x = Date, y = mean_hr_dewpoint_tempF)) +
  geom_point(size = 0.2, alpha = 0.8) + 
  theme_fivethirtyeight_mw() +
  xlab("Date") +
  ylab("Daily Average Dewpoint Temperature (F)") +
  ggtitle("Daily NYC Dewpoint Temperature Trends")

gg_dew_temp +
  geom_smooth(method = "lm", se=FALSE)

ggsave(filename = "gg_dew_temp_joined.png", plot = last_plot(), path = "graphs/", width = 6, height = 4, units = "in", dpi = 300)
```
### Air Pressure

```{r}
gg_pressure <- ggplot(snp_complete_joined_noNA_noRec_interday, aes(x = Date, y = mean_hr_pressure_psi)) +
  geom_point(size = 0.2, alpha = 0.8) + 
  theme_fivethirtyeight_mw() +
  xlab("Date") +
  ylab("Daily Average Air Pressure (PSI)") +
  ggtitle("Daily NYC Air Pressure Trends")

gg_pressure +
  geom_smooth(method = "lm", se=FALSE)

ggsave(filename = "gg_pressure_joined.png", plot = last_plot(), path = "graphs/", width = 6, height = 4, units = "in", dpi = 300)
```

### Wind Speed
```{r}
gg_wind <- ggplot(snp_complete_joined_noNA_noRec_interday, aes(x = Date, y = mean_hr_windspeed_knots)) +
  geom_point(size = 0.2, alpha = 0.8) + 
  theme_fivethirtyeight_mw() +
  xlab("Date") +
  ylab("Daily Average Wind Speed (Knots)") +
  ggtitle("Daily NYC Wind Speed Trends")

gg_wind +
  geom_smooth(method = "lm", se=FALSE)

ggsave(filename = "gg_windspeed_joined.png", plot = last_plot(), path = "graphs/", width = 6, height = 4, units = "in", dpi = 300)
```


### Cloud Cover
```{r}
gg_clouds <- ggplot(snp_complete_joined_noNA_noRec_interday, aes(x = Date, y = mean_hr_clouds_percent * 100)) +
  geom_point(size = 0.2, alpha = 0.8) + 
  theme_fivethirtyeight_mw() +
  xlab("Date") +
  ylab("Daily Average Cloud Coverage (%)") +
  ggtitle("Daily NYC Cloud Coverage Trends")

gg_clouds +
  geom_smooth(method = "lm", se=FALSE)

ggsave(filename = "gg_cloudcover_joined.png", plot = last_plot(), path = "graphs/", width = 6, height = 4, units = "in", dpi = 300)
```


### Precipitation 

```{r}
gg_precip <- ggplot(snp_complete_joined_noNA_noRec_interday, aes(x = Date, y = mean_hr_precip_in * 24)) +
  geom_point(size = 0.2, alpha = 0.8) + 
  theme_fivethirtyeight_mw() +
  xlab("Date") +
  ylab("Daily Total Precipitation (inches)") +
  ggtitle("Daily NYC Precipitation Trends")

gg_precip +
  geom_smooth(method = "lm", se=FALSE)

ggsave(filename = "gg_precip_joined.png", plot = last_plot(), path = "graphs/", width = 6, height = 4, units = "in", dpi = 300)
```


## Time Series

How to test some time series:

http://r-statistics.co/Time-Series-Forecasting-With-R.html

#### Seasonality
It is already well documented that the stock market, including the S&P 500, follows seasonal trends (Saunders, 1993; Kamstra, Kramer, Levi 2012). To handle seasonality effects we included dummy variables for each week of the year (Kamstra et al, 2012).

#### Heteroskedasticity
Interday
```{r}
model <- lm(Return_perc_interday ~ Date, data = snp_complete_joined_noNA_noRec_interday)

ols_test_breusch_pagan(model)
```

Intraday
```{r}
model <- lm(Return_perc_intraday ~ Date, data = snp_complete_joined_noNA_noRec_intraday)

ols_test_breusch_pagan(model)
```

The large Chi^2 (344.9008) indicates that the null hypothesis is false, meaning that variance is not equal - and that the data is, in fact, heteroskedastistic. The original work by Heyes et al overcame similar findings using the Newey West error term, which is used in this as well. However, the Newey West error was not possible to incorporate in the OLS, RandomForest, nor LSTM models. While the machine learning models may adjust for heteroskedasticity to some degree it could be affecting the predictive power of the models overall.

As outlined by Kane, Price, Scotch, and Rabinowitz, ensemble models like RandomForest use “voting” methods that promote higher accuracy aggregate prediction from the constituent trees (2014). These methods were described as some of "the most accurate classification and regression tools currently at the disposal of data scientists" for time series (2014). 

#### Testing for Autocorrelation
First we must do a box test if our model has a "unit root" - which would indicate autocorrelation. Autocorrelation is a problem in time-series analysis, as it means that the data has correlation with its own lagged value. To test for this we can use a Ljung Box Test (Wooldridge, 2009).

Interday
```{r}
Box.test(snp_complete_joined_noNA_noRec_interday$Return_perc_interday, lag = 20, type = 'Ljung-Box')
```


Intraday
```{r}
Box.test(snp_complete_joined_noNA_noRec_intraday$Return_perc_intraday, lag = 20, type = 'Ljung-Box')
```

The p-value of the test, $2.2e^{-16}$, means that we can reject the null hypothesis that there is a unit root, meaning our data is not autocorrelated.

Interday
```{r}
acfRes <- acf(x = select(snp_complete_joined_noNA_noRec_interday, c(Date, Return_perc_interday)), lag.max = 3, type = "correlation", plot = TRUE)

acfRes <- acf(x = select(snp_complete_joined_noNA_noRec_interday, Return_perc_interday), lag.max = nrow(snp_complete_joined_noNA_noRec_interday) - 1, type = "correlation", plot = TRUE, )
```


Intraday
```{r}
acfRes <- acf(x = select(snp_complete_joined_noNA_noRec_intraday, c(Date, Return_perc_intraday)), lag.max = 3, type = "correlation", plot = TRUE)

acfRes <- acf(x = select(snp_complete_joined_noNA_noRec_intraday, Return_perc_intraday), lag.max = nrow(snp_complete_joined_noNA_noRec_interday) - 1, type = "correlation", plot = TRUE, )
```


#### Stationarity
We can use the Augmented Dickey-Fuller Test to see if our data is stationary, or that the "probability distributions are stable over time" (Wooldridge, 2009).

Interday
```{r}
adf.test(snp_complete_joined_noNA_noRec_interday$Return_perc_interday)
```

Intraday
```{r}
adf.test(snp_complete_joined_noNA_noRec_intraday$Return_perc_intraday)

```


The low p-value of 0.01 or lower means we reject the null hypothesis and accept the alternative hypothesis that the data is stationary.


## Predictive Models

Because the models are almost identical we will used the INTERDAY model since we are predicting between days.

### Lagged Data
We need to remove all the current values and only use prior day lags for all of our predictive models.

```{r}
lagged_data <- snp_complete_joined_noNA_noRec_interday %>%
  mutate(VIX_lag = lag(VIX_Close, 1)) %>%
  mutate(ozone_lag = lag(ozone_Daily_8hr_Average_PPM, 1)) %>%
  mutate(CO_lag = lag(CO_Daily_8hr_Average_PPM, 1)) %>%
  mutate(Queens_ozone_lag = lag(`Queens Ozone Data`, 1)) %>%
  mutate(Queens_co_lag = lag(`Queens CO Data`, 1)) %>%
  mutate(dewpoint_lag = lag(mean_hr_dewpoint_tempF, 1)) %>%
  mutate(temp_lag = lag(mean_hr_tempF, 1)) %>%
  mutate(pressure_lag = lag(mean_hr_pressure_psi, 1)) %>%
  mutate(wind_lag = lag(mean_hr_windspeed_knots, 1)) %>%
  mutate(precip_lag = lag(mean_hr_precip_in, 1)) %>%
  mutate(cloud_lag = lag(mean_hr_clouds_percent, 1)) %>%
  select(-c(pm2.5_Daily_Sum_UGPM3, VIX_Close, ozone_Daily_8hr_Average_PPM, CO_Daily_8hr_Average_PPM, `Queens Ozone Data`, `Queens CO Data`, mean_hr_dewpoint_tempF, mean_hr_tempF, mean_hr_pressure_psi, mean_hr_windspeed_knots, mean_hr_precip_in, mean_hr_clouds_percent))

lagged_data <- lagged_data[complete.cases(lagged_data),]
```

### Training & Testing

Mike's suggestions: split it 80/20 but keep sequential order! No random splits.

Good quick tips:
https://machinelearningmastery.com/evaluate-machine-learning-algorithms-with-r/

Caret tips for Time Series:
https://topepo.github.io/caret/data-splitting.html#time

In general we will follow three steps:

+ Train/Test split
+ Cross Validation (5 to 10 folds)
+ Repeated Cross Validation: 5 to 10 fold cross validation and 3 or more repeats to give a more robust estimate

### Model Evaluation Definitions
```{r}
library(caret)
seed <- 4132020
ctrl <- trainControl(method="repeatedcv", number=5, repeats=2)
metric <- "Accuracy"
preProcess=c("center", "scale") # I'm not sure if we should center and scale
```

#### Create Sample

Below is wrong, we need to do it sequentially.
```{r}
# set.seed(seed) # Set Seed so that same sample can be reproduced in future also
# # Now Selecting 80% of data as sample from total 'n' rows of the data  
# sample <- sample.int(n = nrow(lagged_data), size = floor(.8*nrow(lagged_data)), replace = F)
# train <- lagged_data[sample, ]
# test  <- lagged_data[-sample, ]
```


##### Sequential Sample

Since we have 2919 rows in the lagged data we can take the first 2334 (just under 80% of the sample) for training, and use the remaining for testing
```{r}
train <- lagged_data[1:2334, ] # lagged_data$Date[2334] == "2017-06-22"

train <- train %>%
  select(-c(week_53, Wednesday))


test <- lagged_data[2335:nrow(lagged_data), ] # lagged_data$Date[2335] == "2017-06-23"

test <- test %>%
  select(-c(week_53, Wednesday))
```

### Lagged OLS

Now make a predictive OLS model with the lagged data??

#### Create Model

```{r}
summary(train$Date)
summary(test$Date)
```


The model now goes from 2006-12-28 to 2017-4-17 in the training dataset and 2017-4-18 to 2020-1-1 in the testing dataset.
```{r}
full_dates_train <- seq(as.Date("2006-12-27"), as.Date("2017-04-17"), by="days")
full_dates_train <- as.data.frame(full_dates_train)
colnames(full_dates_train)  <- "Date"


full_dates_test <- seq(as.Date("2017-04-18"), as.Date("2020-01-01"), by="days")
full_dates_test <- as.data.frame(full_dates_test)
colnames(full_dates_test)  <- "Date"

train_full <- left_join(full_dates_train, train)

test_full <- left_join(full_dates_test, test)

summary(train_full$Date)
summary(test_full$Date)
```


### Evaluating OLS Time Series Model

4/17, the last date in the testing data, is the 107th day of the calendar.
```{r}
lagged_ts <- ts(train_full[, -1], start = c(2006, 361), end = c(2017, 107), frequency = 365)
lagged_ts <- na.remove(lagged_ts)


model <- lm(Return_perc_interday ~ ., data = lagged_ts)
summary(model)
```

The R Squared is extraordinarily low 0.0476 - or about 4.76% goodness of fit. This kind of makes sense in that we know the that the return percentage fluctuates around a mean fit.

#### OLS Prediction

Do we need to do a split train/test here? R2 gives some accuracy but we would for RMSE, in which case how should we go about sampling?

Lagged_ts_test should really start on 4/18/2017, or the 108th day.
```{r}
lagged_ts_test <- ts(test_full[, -1], start = c(2017, 108), end = c(2020, 1), frequency = 365)
lagged_ts_test <- lagged_ts_test[complete.cases(lagged_ts_test),]

lagged_ts_test_df <- as.data.frame(lagged_ts_test)
lagged_ts_test_df <- lagged_ts_test_df[complete.cases(lagged_ts_test_df), ]

prediction <- predict.lm(model, interval= "prediction", newdata = lagged_ts_test_df)

ols_prediction <- as.data.frame(prediction)
summary(ols_prediction)
```

```{r}
RMSE(pred = ols_prediction$fit, obs = lagged_ts_test_df$Return_perc_interday)
```
We calculate a RMSE score of ~0.8026671, which we will use to compare against the other models for goodness of fit.

```{r}
defaultSummary(data.frame(obs = lagged_ts_test_df$Return_perc_interday, pred = ols_prediction$fit))
```

***Make sure the above things are in the paper!***

#### Fit

```{r}
lm_pred_fit <- test %>%
  select(c(Date, Return_perc_interday)) %>%
  mutate(predictions = ols_prediction$fit)

lm_pred_fit_long <- lm_pred_fit %>%
  gather("key", "value", -Date)

lm_pred_fit_long <- lm_pred_fit_long %>%
  mutate(key = ifelse(key == "Return_perc_interday", "1. True Return %", key)) %>%
  mutate(key = ifelse(key == "predictions", "2. Predicted Return %", key))

# gg_lm_pred <- ggplot(data = lm_pred_fit, aes(x=Date)) + 
#   geom_line(aes(y=Return_perc), color="Return Percent", alpha = 0.5) +
#   geom_line(color="Predictions", aes(y=predictions)) +
#   theme_fivethirtyeight_mw() +
#   xlab("Date") +
#   ylab("S&P Return Value") +
#   scale_color_manual(values=c("Return Percent"='black',"Predictions"='blue')) +
#   ggtitle("OLS - Actual vs Predicted S&P 500 Return %")

library(plyr)

gg_lm_pred <- ggplot(data = lm_pred_fit_long) + 
    theme_fivethirtyeight_mw() +
  geom_line(aes(x=Date, y = value, color = key), alpha = 1, 
             subset = .(key == '2. Predicted Interday Return %')) +
  geom_line(aes(x=Date, y = value, color = key), alpha = 0.5, 
             subset = .(key == '1. True Return %')) +
  scale_color_manual(values = c("darkgrey", "blue"), labels = c("True Interday Return %", "Predicted Interday Return %")) +
  xlab("Date") +
  ylab("S&P Return Value") +
  ggtitle("OLS - Actual vs Predicted S&P 500 Interday Return %")

ggsave(filename = "actualvspredicted_interday_ols.png", plot = gg_lm_pred, path = "graphs/", width = 7, height = 4, units = "in", dpi = 300)

gg_lm_pred

```


### RandomForest

```{r}
library(doMC)
library(caret)
registerDoMC(parallel::detectCores())
```




***START HERE 7.28.2020***



```{r, cache=TRUE}
ctrl <- trainControl(method="repeatedcv", number=5, repeats=2)
# this analysis used repeatedcv with 5 cross validations and repeated 2 times
set.seed(seed)

rf_grid <- data.frame(.mtry = 2:(ncol(lagged_ts) - 1L))

out <- train(Return_perc_interday ~ ., data = lagged_ts, method = "rf", trControl = ctrl, tuneGrid = rf_grid, ntrees = 20, importance = TRUE)

varImp(out)

```

rf variable importance

  only 20 most important variables shown (out of 73)
 
                          Overall
temp_lag	                100.00000			
dewpoint_lag	            89.16561			
Return_perc_interday_lag1	72.96121			
ozone_lag	                69.54356			
pm2.5_Daily_Lag1	        62.58253			
cloud_lag	                60.30806			
pm2.5_Daily_Lag2	        58.11540			
week_51	56.77537			
precip_lag	              56.17486			
CO_lag	                  54.28905				
VIX_lag	                  53.27003			
wind_lag	                51.77204			
pressure_lag	            51.58222			
week_28	                  47.53967			
Queens_co_lag	            46.50938			
tax_dummy	                42.91647			
Return_perc_interday_lag2	42.04882			
week_1	                  41.64503			
week_48	                  41.53217			
week_9	                  41.13664			
20 rows

```{r, cache=TRUE}
library(randomForest)
y_hat <- as.data.frame(predict(out, newdata = lagged_ts_test))

colnames(y_hat) <- "predictions"

lagged_ts_test_df <- as.data.frame(lagged_ts_test)

defaultSummary(data.frame(obs = lagged_ts_test_df$Return_perc_interday, pred = y_hat$predictions))
```

OLD
        RMSE    Rsquared         MAE 
0.790097140 0.000146911 0.539590079 

NEW (7.29.2020)
        RMSE     Rsquared          MAE 
7.894675e-01 9.634951e-05 5.389078e-01

```{r}
rf_comparisons <- test %>%
  select(c(Date, Return_perc)) %>%
  mutate(predictions = y_hat$predictions)


rf_pred_fit_long <- rf_comparisons %>%
  gather("key", "value", -Date)

rf_pred_fit_long <- rf_pred_fit_long %>%
  mutate(key = ifelse(key == "Return_perc", "1. True Return %", key)) %>%
  mutate(key = ifelse(key == "predictions", "2. Predicted Return %", key))


library(plyr)

gg_rf <- ggplot(rf_pred_fit_long) +
  theme_fivethirtyeight_mw() +
  geom_line(aes(x=Date, y = value, color = key), alpha = 1, 
             subset = .(key == '2. Predicted Return %')) +
  geom_line(aes(x=Date, y = value, color = key), alpha = 0.5, 
             subset = .(key == '1. True Return %')) +
  scale_color_manual(values = c("darkgrey", "blue"), labels = c("True Return %", "Predicted Return %")) +
  xlab("Date") +
  ylab("S&P Return Value") +
  ggtitle("RF - Actual vs Predicted S&P 500 Return %")
  
ggsave(filename = "actualvspredicted_rf.png", plot = gg_rf, path = "graphs/", width = 6, height = 4, units = "in", dpi = 300)

gg_rf
```

#### Gather test

```{r}
rf_gathered <- gather(rf_comparisons, "source", "value", -Date)
#head(rf_gathered)

gg_rf <- rf_gathered %>%
  ggplot(aes(x=Date, y = value)) +
  geom_line(aes(color=source)) +
  scale_color_discrete(labels = c("Predicted Return %", "Actual Return %")) +
  theme_fivethirtyeight_mw() +
  xlab("Date") +
  ylab("S&P Return Value") +
  ggtitle("Actual vs Predicted S&P 500 Return Percent") +
  aes(group=rev(source))

ggsave(filename = "actualvspredicted_rf_color.png", plot = gg_rf, path = "graphs/", width = 6, height = 4, units = "in", dpi = 300)

gg_rf
```


### LSTM

I'm using this page as reference:

https://www.r-bloggers.com/time-series-deep-learning-forecasting-sunspots-with-keras-stateful-lstm-in-r/

#### Preprocess Data

We should remove the second an third lagged Return_Perc variables because you don't need them in LSTM.

We'll leave in the one day lags because those are what we're using for all of the variables.

```{r}
train_keras <- train %>%
  mutate(key = "Training")

test_keras <- test %>%
  mutate(key = "Testing")

full_keras <- rbind(train_keras, test_keras)

full_keras <- full_keras %>%
  rename(index = Date) %>%
  rename(value = Return_perc)

write_csv(x = full_keras, path = "LSTM/keras_training_testing.csv", col_names = TRUE)
```


#### LSTM R Packages

```{r}
set.seed(seed)
# Core Tidyverse
library(tidyverse)
library(glue)
library(forcats)

# Time Series
library(timetk)
library(tidyquant)
library(tibbletime)

# Visualization
library(cowplot)

# Preprocessing
library(recipes)

# Sampling / Accuracy
library(rsample)
library(yardstick)

# Modeling
library(keras)
```


#### Check ACF

We need to check the full ACF for auto correlation do determine where to draw the lags - though it may be best to keep this to 2-3 days of lag to make it relatively similar to the other models.

```{r}
tidy_acf <- function(data, value, lags = 0:20) {
    
    value_expr <- enquo(value)
    
    acf_values <- data %>%
        pull(value) %>%
        acf(lag.max = tail(lags, 1), plot = FALSE) %>%
        .$acf %>%
        .[,,1]
    
    ret <- tibble(acf = acf_values) %>%
        rowid_to_column(var = "lag") %>%
        mutate(lag = lag - 1) %>%
        filter(lag %in% lags)
    
    return(ret)
}

max_lag <- nrow(full_keras) - 1

full_keras %>%
    tidy_acf(value, lags = 0:max_lag) %>%
  head()
```

```{r}
full_keras %>%
    tidy_acf(value, lags = 0:max_lag) %>%
    ggplot(aes(lag, acf)) +
    geom_segment(aes(xend = lag, yend = 0)) +
    theme_fivethirtyeight_mw() +
    labs(title = "ACF: S&P 500 Return")
```

```{r}
full_keras %>%
    tidy_acf(value, lags = 0:max_lag) %>%
    ggplot(aes(lag, acf)) +
    geom_segment(aes(xend = lag, yend = 0)) +
    xlim(0, 20) +
    theme_fivethirtyeight_mw() +
    labs(title = "ACF: S&P 500 Return")
```

There's virtually no autocorrelation beyond the first, so we might as well set it to 3 days of lagged values to be consistent with the other models.

NOTE: The rest of the LSTM model was done in Pythong

##### LSTM Graph

This should be run from the LSTM Graph R Markdown file after running the LSTM model in Python.